# Distributed Task Queue partitioning and Rebalancing

üá∫üá∏ A self coordinating distributed task processing system using consistent hashing for dynamic partition assignment and etcd for membership coordination of agents / nodes.

[üá∫üá∏ English](#-english) | [üáßüá∑ Portugu√™s](#-portugu√™s)

## üá∫üá∏ English

### Overview

A distributed task queue where multiple worker processes self coordinate to process tasks from Redis queues without a central scheduler. Workers dynamically discover each other through etcd and use consistent hashing to deterministically decide partition ownership, enabling automatic rebalancing when workers join or leave the cluster.

### Demo

**TODO**: Add video demonstration

### Archicture

**TODO**: Add architecture diagram

#### Core Components

**Redis (Task Storage - queue)**

- 256 fixed partitions (`tasks:0` ... `tasks:255`)
- Tasks hashed by ID to determine partition placement
- Workers use BLPOP for atomic task retrical

**etcd (Membership Coordination)**

- Workers register with 10 second TTL leases
- Continuous lease renewal with KeepAlive
- Watch mechanism detects if a member joins or leaves etcd in real time
- No central coordinator required (furthermore no single point of failure)

**Consistent Hash Ring (partition assignment to workers)**

- 120 virtual nodes for each worker for statistical distribution
- Deterministic partition ownership calculation
- Minimal partition movement during rebalancing (~1/N partitions move when cluster size changes)
- All workers independently will reach the same conclusion about ownership

**Workers (task processors)**

- Self register on startup with a unique ID
- Calculate owned partitions via consistent hashing
- Process tasks from owned partitions using redis BLPOP
- Automatically rebalance when membership changes
- Graceful shutdown with lease revocation

### Key Features

- **Decentralized Coordination** - no single point of failure (a tradeoff from having a central manager of workers...)
- **Dynamic Rebalacing** - Automatic partition redistribution on membership changes
- **At-least-once delivery** - Task guaranteed to be processed even during failures
- **Partition Isolation** - Each task mapped to exactly one partition
- **Graceful Shutdown** - Workers revoke leases before stopping

### Why these Choices

**Why Consistent Hashing**

- **Minimal Movement**: consistent hashing allows only ~1 / N reassigned when adding or removing workers
- **Deterministic**: All workers calculate the same partitions ownership independently
- **No coordination overhead**: No need to communicate partition assignments
- Other approaches (random assignment, range-based, static hashing) either lack dynamic redistribution or require centralized coordination

**Why 256 Fixed Partitions?**

- At I used module on the hash of the task_id to assing the partitions, and as a number of power of 2 enables efficient modulo operations. Altought later on I switched to always get the first byte of the hash, which is still 256 possible combinations. I had to switch because the modulo operator was overflowing the integer.
- It provides a fine grained distribution even with few workers
- Fixed count simplifies reasoning about system behavior
- Its large enough to minime any hot spots in partition distribution across workers, and small enough to avoid unnecessary overhead

**Why pull model (BLPOP)**

- Self-Balancing: fast workers can process more tasks
- No capacity tracking: workers can pull at their own pace
- Atomic operations: Redis BLPOP guaranteers atomic operations, which means one worker gets each task
- Blocks efficiently: no busy waiting or polling overhead

**Why 120 Virtual Nodes?**

- Statistical uniformity: it reduces distribution variance to ~10-15%
- Tested sweet spot: fewer vnodees will lead to uneven distribution, more vnodes = unnecessary overhead

**Why etcd Over Redis for Membership?**

- Etcd is designed for distributed coordination with strong consistency (used by K8s internally)
- It have a built-in lease mechanism with automatic expiration
- The watch API is ideal for tracking real time membership updates
- Raft consensus implemented internally for reliable failure detection

### Technical Challenges Solved

**Race Condition in Rebalancing**

- **Problem**: Multiple goroutines created for each `RunTask()` call competed for rebalance signals, causing goroutine leaks and delayed cancellations
- **Solution**: Single persistent goroutine listens on `updateChan`, cancels processing context and will recreate context after cancellation

**Context Lifecycle Management**

- **Problem**: Reusing cancelled context caused subsequent BLPOP calls to fail immediately
- **Solution**: Recreate context after each cancellation with mutex protection for thread safety

**Graceful Shutdown**

- **Problem**: Workers crashed without notifying cluster, leaving partitions unprocessed until lease expiry (10s)
- **Solution**: Explicit lease revocation on shutdown for immediate partition reassignment

### Quick Start

**Prerequisites**
- Docker and Docker Compose
- Go 1.2x.+

**Setup**
```bash
# start redis and etcd
make setup

# build and run worker:
make execute

# this command create tasks:
make create-tasks
```

**Run Multiple Workers**
```bash
# terminal 1
make execute

# terminal 2  
make execute

# terminal 3
make execute

```

**Clean Up**
```bash
make clean
```

### How It Works

**1. Task Submission**
```
Task with ID -> Hash(ID) -> partition = hash[0] -> Push to redis list tasks:n
```

**2. Worker Startup**
```
Worker starts -> Register in etcd with lease -> Add self to hash ring ->
Calculate owned partitions -> Start BLPOP on owned partition queues
```

**3. Task Processing**
```
BLPOP blocks on owned partitions -> Task arrives -> Process ->
Loop back to BLPOP (continuous processing)
```

**4. Rebalancing**
```
etcd watch detects change -> Update hash ring -> Recalculate partitions ->
Cancel current BLPOP -> Start BLPOP on new partitions
```

**5. Graceful Shutdown**
```
SIGTERM received -> Cancel processing context -> Revoke etcd lease ->
Close connections -> Exit
```

### Project Structure

```
‚îú‚îÄ‚îÄ cmd/
‚îÇ   ‚îú‚îÄ‚îÄ worker/          # worker main entry point
‚îÇ   ‚îî‚îÄ‚îÄ cliTasks/        # cli tool to send tasks
‚îú‚îÄ‚îÄ internal/
‚îÇ   ‚îú‚îÄ‚îÄ worker/          # worker logic & coordination
‚îÇ   ‚îú‚îÄ‚îÄ ring/            # consistent hash ring implementation
‚îÇ   ‚îú‚îÄ‚îÄ conn/            # redis & etcd connection management
‚îÇ   ‚îî‚îÄ‚îÄ types/           # shared types
‚îú‚îÄ‚îÄ docker-compose.yml   # redis & etcd services
‚îî‚îÄ‚îÄ Makefile            # build & run commands
```

### Future Improvements

- [ ] Exponential backoff retry logic
- [ ] Dead letter queue for failed tasks
- [ ] Prometheus metrics (tasks processed, partition ownership, rebalance events)
- [ ] Health check endpoint
- [ ] Tests for membership changes
- [ ] Configurable partition count and virtual nodes
- [ ] A priotity queue for tasks (would be nice to have such)

### Technical Stack

- **Language**: Go 1.21
- **Coordination**: etcd v3.6.7
- **Storage**: Redis 7.x
- **Hashing**: MurmurHash3 (high performance and low collision)
- **Concurrency**: goroutines, channels, mutex, context, sync primitives

## üáßüá∑ Portugu√™s

### Vis√£o geral

Um sistema de processamento de tarefas auto coordenado, usando consistent hashing para atribui√ß√£o de parti√ß√µes de forma din√¢mica e etcd para coorderna√ß√£o de ades√£o de agents / nodes (workers).

Aqui, vamos ter m√∫ltiplas filas no Redis, e vamos ter workers em processos diferentes, se coordenando simultaneamente. Um worker vai pegar uma tarefa da fila, vai processar e depois pegar a pr√≥xima.

### Demonstra√ß√£o

**TODO**: Adicionar v√≠deo de demonstra√ß√£o

### Arquitetura

**TODO**: Adicionar diagrama de arquitetura

#### Componentes Principais

**Redis (armazenamento de tarefas - fila)**

Redis guarda 256 filas separadas chamadas `tasks:0` at√© `tasks:255`. Quando uma tarefa chega, fazemos hash do ID dela e usamos o primeiro byte desse hash para descobrir em qual fila ela vai (sempre ter√° um valor entre 0 e 255). Os workers usam BLPOP, que √© uma opera√ß√£o at√¥mica do Redis que pega uma tarefa da fila e bloqueia esperando se a fila estiver vazia.

**etcd (coordena√ß√£o de membros)**

Cada worker se registra no etcd com um lease de 10 segundos que fica sendo renovado continuamente atrav√©s do KeepAlive. O etcd tem um mecanismo de watch que notifica em tempo real quando algum worker entra ou sai. N√£o existe coordenador central, ent√£o n√£o tem ponto √∫nico de falha.

**Consistent Hash Ring (atribui√ß√£o de parti√ß√µes aos workers)**

O hash ring usa 120 n√≥s virtuais para cada worker para melhorar a distribui√ß√£o estat√≠stica das parti√ß√µes. Todos os workers fazem o mesmo c√°lculo de forma independente e chegam na mesma conclus√£o sobre quem √© dono de quais parti√ß√µes. Quando o tamanho do cluster muda, apenas cerca de 1/N das parti√ß√µes precisam ser movidas para outros workers.

**Workers (processadores de tarefas)**

Cada worker se registra na inicializa√ß√£o com um ID √∫nico, calcula quais parti√ß√µes ele deve processar usando consistent hashing, e come√ßa a fazer BLPOP nessas parti√ß√µes. Quando a composi√ß√£o do cluster muda, ele automaticamente recalcula suas parti√ß√µes. No shutdown, ele revoga seu lease do etcd antes de parar.

### Funcionalidades Principais

**Coordena√ß√£o descentralizada**: n√£o existe ponto √∫nico de falha porque n√£o tem gerenciador central de workers
**Rebalanceamento din√¢mico**: quando workers entram ou saem, as parti√ß√µes s√£o redistribu√≠das automaticamente
**Entrega ao menos uma vez**: tarefas s√£o garantidas de serem processadas mesmo quando acontecem falhas
**Isolamento de parti√ß√µes**: cada tarefa vai para exatamente uma parti√ß√£o espec√≠fica
**Shutdown gracioso**: workers revogam seus leases antes de parar

### Por Que Essas Escolhas

**Por que Consistent Hashing**

- Quantidade m√≠nima de opera√ß√µes: consistent hashing garante que apenas cerca de 1/N das parti√ß√µes sejam reatribu√≠das quando adicionamos ou removemos workers
- Determin√≠stico: todos os workers calculam as mesmas atribui√ß√µes de parti√ß√µes de forma independente
- Sem overhead de coordena√ß√£o: n√£o precisa comunicar atribui√ß√µes de parti√ß√µes entre workers
- Outras abordagens como random distribution, baseada em ranges, ou static hashing ou n√£o permitem redistribui√ß√£o din√¢mica ou precisam de coordena√ß√£o centralizada

**Por que 256 parti√ß√µes fixas**

- Inicialmente eu estava usando m√≥dulo no hash do task_id para atribuir parti√ß√µes, e como 256 √© pot√™ncia de 2, isso permite opera√ß√µes de m√≥dulo eficientes. Depois mudei para sempre pegar o primeiro byte do hash, que ainda d√° 256 combina√ß√µes poss√≠veis. Tive que mudar porque a opera√ß√£o de m√≥dulo estava causando overflow no inteiro
- Fornece distribui√ß√£o granular mesmo com poucos workers
- N√∫mero fixo simplifica o racioc√≠nio sobre comportamento do sistema
- √â grande o suficiente para minimizar pontos quentes na distribui√ß√£o de parti√ß√µes entre workers, e pequeno o suficiente para evitar overhead desnecess√°rio

**Por que modelo pull com BLPOP**

- Auto balanceamento: workers r√°pidos naturalmente processam mais tarefas
- Sem rastreamento de capacidade: workers pegam tarefas no pr√≥prio ritmo
- Opera√ß√µes at√¥micas: Redis BLPOP garante opera√ß√µes at√¥micas, ou seja, apenas um worker pega cada tarefa
- Bloqueia eficientemente: n√£o tem busy waiting nem overhead de polling

**Por que 120 n√≥s virtuais**

- Uniformidade estat√≠stica: reduz varia√ß√£o de distribui√ß√£o para cerca de 10 a 15%
- Ponto ideal: menos n√≥s virtuais leva a distribui√ß√£o desigual, mais n√≥s virtuais causa muito overhead

**Por que etcd ao inv√©s de Redis para membership**

- Etcd foi projetado para coordena√ß√£o distribu√≠da com consist√™ncia forte (usado internamente pelo K8s)
- Tem mecanismo de lease integrado com expira√ß√£o autom√°tica
- A API de watch √© ideal para rastrear atualiza√ß√µes de membership em tempo real
- Raft Consensus implementado internamente para detectar de falhas

### Desafios T√©cnicos Resolvidos

**Race condition no rebalanceamento**

**Problema**: m√∫ltiplas goroutines eram criadas a cada chamada de `RunTask()` e competiam pelos sinais de rebalanceamento, causando vazamento de goroutines e cancelamentos atrasados
**Solu√ß√£o**: uma √∫nica goroutine persistente escuta no `updateChan` cancela o contexto de processamento e recria o contexto ap√≥s cancelamento

**Gerenciamento de ciclo de vida do contexto**

**Problema**: reusar contexto cancelado fazia as pr√≥ximas chamadas de BLPOP falharem imediatamente
**Solu√ß√£o**: recriar contexto ap√≥s cada cancelamento com prote√ß√£o de mutex para thread safety

**Shutdown gracioso**

**Problema**: workers crashavam sem notificar o cluster, deixando parti√ß√µes sem processar at√© o lease expirar (10 segundos)
**Solu√ß√£o**: revoga√ß√£o expl√≠cita do lease no shutdown para reatribui√ß√£o imediata de parti√ß√µes

### Como Come√ßar

**Pr√© requisitos**

Docker e Docker Compose

Go 1.2x ou superior

**Configura√ß√£o**
```bash

## License

MIT License - feel free to use this project for learning and interviews.

## Author

Victor Reis - [my website](https://viquitorreis.github.io/)