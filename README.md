# Distributed Task Queue partitioning and Rebalancing

## ğŸ‡ºğŸ‡¸ EN


## ğŸ‡§ğŸ‡· PT-BR

## O que Ã©

Ã‰ uma fila de tarefas distribuÃ­das, que automaticamente se particiona e faz rebalanceamento.

Aqui, vamos ter mÃºltiplas filas no Redis, e vamos ter workers em processos diferentes, se coordenando simultaneamente. Um worker vai pegar uma tarefa da fila, vai processar e depois pegar a prÃ³xima.

### <span style="color:rgb(0, 255, 136)">CenÃ¡rio</span>

Imagina que temos um sistema que precisa processar **milhares de tarefas por segundo.** Como:

- Processar imagens
- Enviar e-mails
- Gerar relatÃ³rios

Qualquer tarefa que pode demorar algum tempo. Se fizermos isso em apenas um servidores, ele nÃ£o daria conta, ia entrar em sobrecarga, dependendo do nÃ­vel de escala. EntÃ£o, precisamos distribuir esse trabalho entre vÃ¡rios servidores.

### <span style="color:rgb(0, 255, 136)">Problemas</span>

Mas existem **problemas**:

- Como coordenar isso?
- Como garantimos que cada tarefa seja processada exatamente uma vez?
- Como dividimos o trabalho de forma justa entre os servidores?
- O que acontece quando um servidor morre ou quando adicionamos servidores novos?

### Abordagens possÃ­veis

Podemos resolver esses problemas das seguintes formas:

1. Ter um Coordenador central, uma entidade centralizada que coordena todos os trabalhos. PorÃ©m tem um problema nessa abordagem, essa entidade central Ã© um ponto Ãºnico de falha e um gargalo ao sistema. Se ele falhar o sistema todo falha e perdemos em **disponibilidade.** (availability - CAP)
2. A segunda abordagem Ã© ter um sistema onde os prÃ³prios workers se coordenam entre si, sem entidade central, se rebalanceando de forma dinÃ¢mica e inteligente. Essa Ã© a abordagem desse projeto.

### Ideias Centrais desse projeto

Aqui temos 3 ideias centrais:

1. Usar **particionamento:** dividir todas as tarefas possÃ­veis em 256 grupos fixos, chamados **partitions**.
2. Usar **consistent hashing:** para decidir qual worker Ã© responsÃ¡vel por quais partitions de forma determinÃ­stica. Ou seja, um roteamento preciso para o sistema.
3. Usar **etcd**: para os workers se descobrirem e coordenarem quem estÃ¡ vivo no cluster.

Dessa forma, os **workers** podem olhar ao etcd e ver quem estÃ¡ vivo, roda o mesmo algoritmo de consistent hashing e todos chegam na mesma conclusÃ£o sobre quem deve processar quais partitions. Ã‰ um **consenso - consensus protocol**.

### ImplementaÃ§Ã£o

#### Consistent Hashing

Para distribuir e redistribuir as tarefas entre os nodes (workers), de forma dinÃ¢mica foi escolhido o Consistent Hashing. Existem outras formas para se chegar ao particionamento, porÃ©m essa estrutura de dados se mostra otima nesse cenÃ¡rio de redistribuiÃ§Ã£o dinÃ¢mica, onde precisamos de sincronizaÃ§Ã£o entre os nodes e eles podem parar ou falhar...

##### Fluxo

**Fluxo CriaÃ§Ã£o de Task**

.:. Aqui nÃ£o temos o consistent hashing, apenas Ã  tÃ­tulo de explicar a dinÃ¢mica...

1. Task chega com um ID / nome
2. Ã‰ feito um hash a partir do ID / nome da task
3. Se faz partition = hash % 256 (quantidade de partitions). O valor resultante desse restante serÃ¡ a partition de destino na fila.
4. Push na fila, com a task na partition correta.

**Descobrindo quais partitions e tasks sÃ£o do worker**

No consistent hashing... Worker side.

fluxo: vnode -> partition -> tasks

**Big O**: O(P log V), onde P = partitions, V = vnodes.
    Acontece apenas quando mudanÃ§a de membership ocorrem (bootstrap de worker exclusÃ£o de worker), entÃ£o o overhead Ã© mÃ­nimo comparado com o throughput de processamento de tasks.

Para um worker qualquer (e.g. worker-A) saber o que deve ser processado, ele irÃ¡:

1. Chamar GetPartitionsForNode("worker-A)
2. GetPartitionsForNode faz 256 iteraÃ§Ãµes (ou quantas partitions tivermos - isso sÃ³ ocorre quando um novo worker entra ou sai do etcd).
    Para a partition 0:
    - Hash "partition:0" -> obtem hash 123456
    - Busca no ring o **primeiro vnode >= 123456** (primeiro vnode hash >= hash da partition desejada).
    - Encontra vnode hash 150000 -> esse vnode pertence a "worker-B"
    - Partition 0 NÃƒO Ã© do worker-A, Ã© de outro, entÃ£o vai ignorar

    ...

    Para partition 42:
    - Hash "partition:42" -> obtÃ©m hash 999999
    - Busca no ring o primeiro vnode >= 999999
    - Encontra vnode com hash 1200000 -> esse vnode sim pertence a "worker-A"

    ... itera 256 vezes (por todas partitions)
3. Retorna: [5, 7, 23, 42, 58, ...] <- todas partitions que pertencem a worker-A
4. Worker-A faz BLPOP em ["tasks:5", "tasks:7", "tasks:23", "tasks:42", ...]
5. Tasks que chegarem nessas partitions no Redis (pela fila) sÃ£o processadas pelo worker-A sempre.

##### Trade-offs:

**Random Assingment**

Nesse formato, os dados seriam distribuidos de forma randÃ´mica. PorÃ©m, o client nÃ£o sabe em qual node estÃ¡ o nÃ³, nÃ£o sendo possÃ­vel a re-distribuiÃ§Ã£o dinÃ¢mica.

**Cache Global Ãšnico**

Essa estrutura Ã© rÃ¡pida. PorÃ©m tem problemas de escala e fica complexo administrar devido ao cache miss.

**Key Range** - intervalo de chaves

O client pode obter facilmente os dados do server, porÃ©m a distribuÃ§Ã£o de chaves entre os nodes nÃ£o Ã© boa, que pode sobrecarregar um node especifico e deixando outros ociosos.

**Static Hashing**

Usa um identificador, como IP ou algo do tipo. Os nodes sÃ£o armazenados em um array e o mÃ³dulo do hash service compuita o hash e a chave de dados. Problema: se um node quebrar, Ã© complicado para redistribuir as partiÃ§Ãµes entre os nodes que ainda existem. Adicionar novos nodes tambÃ©m Ã© complexo.

**Consistent Hashing**

Essa Ã© uma estrutura inteligente, ela tem um hash ring que minimiza o nÃºmero de chaves para ser remapeado quando o nÃºmero de nodes mudarem. Basicamente, a estrutura de anel terÃ¡ cada node posicionado em um local dele, e nossa funÃ§Ã£o hash vai mapear para uma posiÃ§Ã£o do anel. Ao chegar no anel, se NÃ£o encontrou o node diretamente, a estrutura Ã© atravessada em sentido horÃ¡rio do anel, atÃ© encontrar o node mais pÅ•oximo. Se um node quebrar, tudo que era de sua responsabilidade passa para o prÃ³ximo node (que jÃ¡ Ã© a travessia em si).

Essa estrutura Ã© bascamente uma **BST - Binary Search Tree**. Dessa forma conseguimos inserÃ§Ã£o, busca e remoÃ§Ã£o em **O(log n)**.

##### Algoritmos de Hash

Inicialmente, foram considerados 3 algoritmos para a hash function:

- SHA256
- Murmur3
- MD5
Todos sÃ£o amplamente utilizados em tecnologia da informaÃ§Ã£o.

**SHA256**

O SHA256 Ã© uma funÃ§Ã£o Hash criptogrÃ¡fica que produz um tamanho fixo de 256 bits (32 bytes) de hash value a partir de um input de tamanho arbitrÃ¡rio. Ele Ã© muito seguro, usado em protocolos de rede e sistemas, SSL/TLS, assinaturas digitais e blockchains.

Alguns usos comuns seriam:

1. Cryptomoedas
2. Integridade de dados
3. Assinaturas digitais
4. Storage de senhas

**Murmur3**

O MurmurHash Ã© uma funÃ§Ã£o hash nÃ£o criptogrÃ¡fica, foi feita para ser rÃ¡pido e eficiente, usado prioritariamente para estruturas que dependem de hash tables onde tem uma performance Ã³tima nessas operaÃ§Ãµes. Ele fornece baixa colisÃ£o de dados e altÃ­ssima distribuiÃ§Ã£o dos valores de hash. Apesar de nÃ£o ser criptogrÃ¡fico, tem alta performance e eficiÃªncia, um dos motivos de ser usado no Apache Cassandra.

1. Hash tables
2. Processamento de dados
3. IndexaÃ§Ã£o de banco de dados
4. Load Balancing

**MD5**

TambÃ©m uma funÃ§Ã£o hash muito usada, tem um valor de hash de 128 bits que resultam em 32 caracteres. Apesar de ter sido criado para criptografia, se mostrou nÃ£o ser tÃ£o seguro assim. Esse algoritmo, nÃ£o oferece a melhor performance, e nem uma seguranÃ§a otima, e por isso foi descartado.

Portanto, o algoritmo escolhido foi o **murmur3**, tendo em vista que neste cenÃ¡rio de sistemas distribuÃ­dos, performance se torna crÃ­tico.

##### Partitioning

##### Virtual Nodes

Um conceito muito utilizado hoje em dia ao se fazer consistent hashing, Ã© fazer o uso de **virtual nodes** (vnodes). Vnodes permitem que cada node (no nosso caso os workers) seja dono de um grande nÃºmero de intervalos de particionamentos, distribuidos ao longo do da Hash Ring.

![alt text](./assets/image.png)

referÃªncia: https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/architecture/archDataDistributeDistribute.html

Os virtual nodes sÃ£o distribuÃ­dos ao longo do ring, e cada virtual node estarÃ¡ atrelado a um worker.

Quando fazemos o hash de uma partition, vamos sempre buscar o primeiro VNode com o hash >= que o hash daquela partition. Achado o VNode, retornamos qual Ã© seu Worker corretamente.